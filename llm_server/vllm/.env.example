NAME=some_name
HF_TOKEN=some_hf_token
CUDA_VISIBLE_DEVICES=0,1 # can be single GPU like 0 or 1 or multiple GPUs like 0,1,2
HOST=0.0.0.0 # set to host IP address to be accessible from other machines
PORT=8001 # port for the vLLM server
NUM_GPU=2 # number of GPUs to use
MODEL_NAME=SOME_MODEL_NAME # e.g., Qwen/Qwen3-14B-FP8
MEM_UTILIZATION=0.85 # fraction of GPU memory to use
MAX_MODEL_LEN=32768 # maximum model context length
MAX_NUM_SEQS=16 # maximum number of sequences to process in parallel
# Open web UI
WEB_PORT=8002 # port for the web UI

